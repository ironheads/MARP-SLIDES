# 生成模型中的扩散模型

## 前置知识

### 条件概率

$$
P(A,B,C)=P(C|B,A)P(B,A)=P(C|B,A)P(B,A)P(A)\\
$$

$$
P(B,C|A)=P(B|A)P(C|A,B)
$$

### 马尔可夫假设下的条件概率

$$
P(A,B,C)=P(C|B)P(B|A)P(A)\\
P(B,C|A)=P(B|A)P(C|B)
$$

### 高斯分布的KL散度

$$
KL(p,q)=\log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2}-
\frac{1}{2}
$$

### 重参数化技巧

假设我们要训练一个模型预测高斯分布的均值和方差。

直接从$\mathcal{N}(\mu,\sigma)$采样后如何传递梯度？无法传递。



所以直接在$\mathcal{N}(0,1)$上采样$z$，通过变换$x=\mu+ \sigma \cdot z$得到采样点，随机性变为$z$。

##  生成模型

### 是什么？作用？

​		生成模型：能够随机生成观测数据的模型。

​		方法：学习真实数据分布，$p_{data}(x)$。

​		如何生成数据：在$p_{data}$中采样即可。

### 如何建模数据分布？

​		显然实际的数据分布$p_{data}$是无法求解的。我们只能建模一个$P_{\theta}(x)$拟合真实的数据分布$p_{data}(x)$。

​		$p_{model}(x;\theta)$为模型的参数为$\theta$时描述的数据分布。任务目标则是寻找最好的参数$\theta ^{\ast}$，使$p_{model}(x;\theta ^ {\ast})$与真实分布$p_{data}(x)$最接近。
$$
\theta^{*}=\underset{\theta}{\operatorname{\arg\min}} D_{K L}\left(p_{d a t a}(x) \| p_{\text {model }}(x ; \theta)\right)
$$
​		最大似然估计，真实样本应当在实际分布中出现的概率足够大。
$$
\begin{aligned}
&\theta^{*}=\underset{\theta}{\operatorname{argmax}} \prod_{i=1}^{m} p_{\text {model }}\left(x^{(i)} ; \theta\right) \\
&=\underset{\theta}{\operatorname{argmax}} \log \prod_{i=1}^{m} p_{\text {model }}\left(x^{(i)} ; \theta\right) \\
&=\underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{m} \log p_{\text {model }}\left(x^{(i)} ; \theta\right)
\end{aligned}
$$

$$
\boldsymbol{\theta}^{*}=\underset{\boldsymbol{\theta}}{\arg \max } \mathbb{E}_{x \sim p_{\text {data }}} \log p_{\text {model }}(\boldsymbol{x} \mid \boldsymbol{\theta})
$$

### 模型分类（来自深度学习花书）

![img](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/generative-overview-20220531204248467.png)

<img src="https://pic4.zhimg.com/80/v2-722f6bdd709fe452c1447ae572bcc28b_1440w.jpg" alt="img" style="zoom:40%;" />

#### 显性密度模型

显性的定义密度分布$p_{\theta}(x)$

##### Tractable explicit model（易解显性模型）

​	主要一类模型是**Fully visible belief nets**，简称FVBN，也被称作**Auto-Regressive Network**。
$$
p_{\text {model }}(\vec{x})=\prod_{i=1}^{n} p_{\text {model }}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)
$$
​	这玩意怎么做？自回归模型——RNN或者mask。

##### Approximate explicit model（近似显性模型）

>n维的数据分布可以通过一个同样是n维的标准高斯分布+一个足够复杂的函数映射得到。
>
>基于流的生成模型，感兴趣可以看一下，这个应该应该也属于这一类，通过可逆的变换直接最小化KL散度，不像VAE一样最小化ELBO。可以看李宏毅老师的课程[Flow-based Generative Model - YouTube](https://www.youtube.com/watch?v=uXY18nzdSsM&t=2983s)，课程中对于一些矩阵，线性代数的讲解还挺有意思的。
>
>允许损失一些精度的话,高斯分布的维度可以少一些。

###### 变分近似 （VAE）

​	想要根据一个latent code $z$生成一个数据。观测的是样本数据$x$。 我们假设latent code $z$的分布为标准正态分布。
$$
p_{\theta}(x)=\sum_{Z}{p_{\theta}(x|z)p(z)}
$$
​	$p_{\theta}(x|z)$可以用一个input为$z$的神经网络去估计。

​	还是不能求。

$$
p_{\theta}(z|x)=\frac{p_{\theta}(x|z)p(z)}{p_{\theta}(x)} \\
p_\theta(x)=\frac{p_{\theta}(x|z)p(z)}{p_{\theta}(z|x)}
$$

用网络$q_\phi(z|x)$估计$P_\theta(z|x)$

<img src="%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220528224501296.png" alt="image-20220528224501296" style="zoom:50%;" />



<img src="%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220528224401929.png" alt="image-20220528224401929" style="zoom:50%;" />

###### 玻尔兹曼机

​		假设样本分布遵循玻尔兹曼分布，提出的随机递归神经网络。

​		属于比较早期的网络，不详细介绍。

####  隐性密度模型

##### 直接隐性密度模型

​	我们放弃估计概率密度，只想要能采样就行。
$$
\theta:=\min _{\theta} D\left(p_{\theta}(x), p_{\hat{x}}(\hat{x})\right)
$$
<img src="%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220528230704940.png" alt="image-20220528230704940" style="zoom:50%;" />

​	不多赘述。

##### 	马尔可夫链

生成随机网络

![image-20220529004001434](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220529004001434.png)

##  扩散模型

![img](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/DDPM.png)

![img](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/diffusion-example-20220531211230136.png)

### 扩散过程

在图片上一步步加噪的过程。不断网分布中加高斯噪声，这里添加的噪声是标准差为$\beta_t$的高斯噪声。加噪声的过程是一个马尔可夫过程。

![img](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/DDPM.png)

随着加噪声的次数不断增大，最终数据分布变为了一个各向独立的高斯分布。
$$
q(\mathbf{x}_t|\mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x_t};\sqrt{1-\beta_t}\mathbf{x_{t-1}}.\beta_t\mathbf{I})\\
q(\mathbf{x}_{1:T}|\mathbf{x}_0)=\prod \limits_{i=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1})
$$
任意时刻的$q(\mathbf{x}_t|\mathbf{x}_{0})$可以直接计算出来。

令$\alpha_t=1-\beta_t,\bar{\alpha}_t=\prod_{i=1}^{t}\alpha_i$
$$
\begin{aligned}
\mathbf{x}_{t} &=\sqrt{\alpha_{t}} \mathbf{x}_{t-1}+\sqrt{1-\alpha_{t}} \mathbf{z}_{t-1} \\
&=\sqrt{\alpha_{t}} (\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{1-\alpha_{t-1}} \mathbf{z}_{t-2} ) +\sqrt{1-\alpha_{t}} \mathbf{z}_{t-1} \\
&=\sqrt{\alpha_{t} \alpha_{t-1}} \mathbf{x}_{t-2}+\sqrt{1-\alpha_{t} \alpha_{t-1}} \overline{\mathbf{z}}_{t-2} \\
&=\cdots \\
&=\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \mathbf{z} \\
q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right) &=\mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0},\left(1-\bar{\alpha}_{t}\right) \mathbf{I}\right)
\end{aligned}
$$

### 逆扩散过程（去躁过程）

​		从噪声中恢复原始数据

​		可以假设$q(\mathbf{x}_{t-1}|\mathbf{x}_{t})$也是一个高斯分布。但是我们没有办法去拟合。构造出参数分布去估计。
$$
p_{\theta}\left(\mathbf{x}_{0: T}\right)=p\left(\mathbf{x}_{T}\right) \prod_{t-1}^{T} p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right) \\
p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \mathbf{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)
$$

### 公式推导

#### 后验条件概率分布$q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_0)$

​			显然我们无法知道$q(\mathbf{x}_{t-1}|\mathbf{x}_{t})$到底是一个什么分布。我们明显不可能遍历所有加噪的可能去训练$q(\mathbf{x}_{t-1}|\mathbf{x}_{t})$。

但是我们可以计算出后验条件概率分布$q(\mathbf{x}_{t-1} \mid \mathbf{x}_{t},\mathbf{x}_0)$

假设
$$
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \tilde{\boldsymbol{\mu}}\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right), \tilde{\beta}_{t} \mathbf{I}\right)
$$

这是因为

$$
\begin{aligned}
q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}, \mathbf{x}_{0}\right) & = \frac{q\left( \mathbf{x}_{t-1} , \mathbf{x}_{t} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}\frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)} \\
&=q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}, \mathbf{x}_{0}\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)} \\
&=q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right) \frac{q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{0}\right)}{q\left(\mathbf{x}_{t} \mid \mathbf{x}_{0}\right)} \\
& \propto \exp \left(-\frac{1}{2}\left(\frac{\left(\mathbf{x}_{t}-\sqrt{\alpha_{t}} \mathbf{x}_{t-1}\right)^{2}}{\beta_{t}}+\frac{\left(\mathbf{x}_{t-1}-\sqrt{\alpha_{t-1}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t-1}}-\frac{\left(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}\right)^{2}}{1-\bar{\alpha}_{t}}\right)\right) \\
&=\exp \left(-\frac{1}{2}\left(\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \mathbf{x}_{t-1}^{2}-\left(\frac{2 \sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{2 \sqrt{\bar{\alpha}_{t}}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0}\right) \mathbf{x}_{t-1}+C\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)\right)\right)
\end{aligned}
$$

$$
\begin{aligned}
\tilde{\beta}_{t} &= 1 /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \\
& = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}} \cdot \beta_{t} \\
\tilde{\boldsymbol{\mu}}_{t}\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right) & =\left(\frac{\sqrt{\alpha_{t}}}{\beta_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t}}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0}\right) /\left(\frac{\alpha_{t}}{\beta_{t}}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \\ 
&=\frac{\sqrt{\alpha_{t}}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_{t}} \mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_{t}}{1-\bar{\alpha}_{t}} \mathbf{x}_{0} \\
&=\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \mathbf{z}_{t}\right)
\end{aligned}
$$

### 目标数据分布的似然函数

$$
\begin{aligned}
-\log p_{\theta}\left(\mathbf{x}_{0}\right) & \leq-\log p_{\theta}\left(\mathbf{x}_{0}\right)+D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right) \| p_{\theta}\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)\right) \\
&=-\log p_{\theta}\left(\mathbf{x}_{0}\right)+\mathbb{E}_{\mathbf{x}_{1: T} \sim q(\mathbf{x}_{1:T} \mid \mathbf{x}_{0}) }\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right) / p_{\theta}\left(\mathbf{x}_{0}\right)}\right] \\
&=-\log p_{\theta}\left(\mathbf{x}_{0}\right)+\mathbb{E}_{q}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}+\log p_{\theta}\left(\mathbf{x}_{0}\right)\right] \\
&=\mathbb{E}_{q}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}\right] \\
\text { Let } L_{\mathrm{VLB}} &=\mathbb{E}_{ q \left( \mathbf{x}_{0: T} \right)}\left[\log \frac{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}{p_{\theta}\left(\mathbf{x}_{0: T}\right)}\right] \geq-\mathbb{E}_{q\left(\mathbf{x}_{0}\right)} \log p_{\theta}\left(\mathbf{x}_{0}\right)
\end{aligned}
$$

处理这个上界
$$
\begin{aligned}  L_\text{VLB}  &= \mathbb{E}_{q(\mathbf{x}_{0:T})} \Big[ \log\frac{q(\mathbf{x}_{1:T}\vert\mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \Big] \\  &= \mathbb{E}_q \Big[ \log\frac{\prod_{t=1}^T q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{ p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t) } \Big] \\  &= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} \Big] \\  &= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t\vert\mathbf{x}_{t-1})}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\  &= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \Big( \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)}\cdot \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1}\vert\mathbf{x}_0)} \Big) + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\  &= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_t \vert \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} + \log\frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big] \\  &= \mathbb{E}_q \Big[ -\log p_\theta(\mathbf{x}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} + \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{q(\mathbf{x}_1 \vert \mathbf{x}_0)} + \log \frac{q(\mathbf{x}_1 \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)} \Big]\\  &= \mathbb{E}_q \Big[ \log\frac{q(\mathbf{x}_T \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t)} - \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1) \Big] \\  &= \mathbb{E}_q [\underbrace{D_\text{KL}(q(\mathbf{x}_T \vert \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) \parallel p_\theta(\mathbf{x}_{t-1} \vert\mathbf{x}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{x}_0 \vert \mathbf{x}_1)}_{L_0} ]  \end{aligned}  \\
$$

$$
\begin{aligned}
p_{\theta}\left(\mathbf{x}_{0} \mid \mathbf{x}_{1}\right) &=\prod_{i=1}^{D} \int_{\delta_{-}\left(x_{0}^{i}\right)}^{\delta_{+}\left(x_{0}^{i}\right)} \mathcal{N}\left(x ; \mu_{\theta}^{i}\left(\mathbf{x}_{1}, 1\right), \sigma_{1}^{2}\right) d x \\
\delta_{+}(x) &=\left\{\begin{array}{ll}
\infty & \text { if } x=1 \\
x+\frac{1}{255} & \text { if } x<1
\end{array} \quad \delta_{-}(x)= \begin{cases}-\infty & \text { if } x=-1 \\
x-\frac{1}{255} & \text { if } x>-1\end{cases} \right.
\end{aligned}
$$

###  损失函数的处理

我们要的是什么？
$$
p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \mathbf{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\\
\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)=\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \mathbf{z}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\\
\mathbf{x}_{t-1}=\mathcal{N}\left(\mathbf{x}_{t-1} ; \frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \mathbf{z}_{\theta}\left(\mathbf{x}_{t}, t\right)\right), \mathbf{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)
$$
损失函数 
$$
\begin{aligned}
L_{t} &=\mathbb{E}_{\mathbf{x}_{0}, \mathbf{z}}\left[\frac{1}{2\left\|\boldsymbol{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|_{2}^{2}}\left\|\tilde{\boldsymbol{\mu}}_{t}\left(\mathbf{x}_{t}, \mathbf{x}_{0}\right)-\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}\right] \\
&=\mathbb{E}_{\mathbf{x}_{0}, \mathbf{z}}\left[\frac{1}{2\left\|\boldsymbol{\Sigma}_{\theta}\right\|_{2}^{2}}\left\|\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \mathbf{z}_{t}\right)-\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \mathbf{z}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\right\|^{2}\right] \\
&=\mathbb{E}_{\mathbf{x}_{0}, \mathbf{z}}\left[\frac{\beta_{t}^{2}}{2 \alpha_{t}\left(1-\bar{\alpha}_{t}\right)\left\|\mathbf{\Sigma}_{\theta}\right\|_{2}^{2}}\left\|\mathbf{z}_{t}-\mathbf{z}_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}\right] \\
&=\mathbb{E}_{\mathbf{x}_{0}, \mathbf{z}}\left[\frac{\beta_{t}^{2}}{2 \alpha_{t}\left(1-\bar{\alpha}_{t}\right)\left\|\mathbf{\Sigma}_{\theta}\right\|_{2}^{2}}\left\|\mathbf{z}_{t}-\mathbf{z}_{\theta}\left(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \mathbf{z}_{t}, t\right)\right\|^{2}\right]
\end{aligned}
$$

### 训练方法

![image-20220601222212220](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220601222212220-4093334.png)

### 训练设置

![image-20220601225009854](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220601225009854-4095011.png)

### 效果

![image-20220601225835436](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220601225835436-4095517.png)

![image-20220601225933121](%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B.assets/image-20220601225933121.png)

### 一些拓展

#### Guided Diffusion - 基于类别引导的扩散模型

在DDPM中，无条件的逆向过程可以用 $p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)$描述，在加入类别条件$y$后，逆向过程可以表示为
$$
p_{\theta, \phi}\left(x_{t-1} \mid x_{t}, y\right)=Z \cdot p_{\theta}\left(x_{t-1} \mid x_{t}\right) \cdot p_{\phi}\left(y \mid x_{t-1}\right)
$$
生成方式
$$
\mu, \Sigma \leftarrow \mu_{\theta}\left(x_{t}\right), \Sigma_{\theta}\left(x_{t}\right)
$$
$x_{t-1} \leftarrow$ sample from $\mathcal{N}(\mu+s \Sigma g, \Sigma)$, where $g=\nabla_{x_{t}} \log \left(p_{\phi}\left(y \mid x_{t}\right)\right)$

